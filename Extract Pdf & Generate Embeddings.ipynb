{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9ca2c2f-d6fd-4fb2-80bf-f665c0cd921e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Mounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "407d12e8-9555-4b25-82ca-6910c7f09c9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mount succeeded!\n"
     ]
    }
   ],
   "source": [
    "storageAccountName = \"\"\n",
    "storageAccountAccessKey = ''\n",
    "sasToken = ''\n",
    "blobContainerName = \"rag-files\"\n",
    "mountPoint = \"/mnt/data/\"\n",
    "if not any(mount.mountPoint == mountPoint for mount in dbutils.fs.mounts()):\n",
    "  try:\n",
    "    dbutils.fs.mount(\n",
    "      source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n",
    "      mount_point = mountPoint,\n",
    "      extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n",
    "    )\n",
    "    print(\"mount succeeded!\")\n",
    "  except Exception as e:\n",
    "    print(\"mount exception\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4817798-c4f9-481f-9cdf-820e5a3d315b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[MountInfo(mountPoint='/databricks-datasets', source='databricks-datasets', encryptionType=''),\n",
       " MountInfo(mountPoint='/Volumes', source='UnityCatalogVolumes', encryptionType=''),\n",
       " MountInfo(mountPoint='/databricks/mlflow-tracking', source='databricks/mlflow-tracking', encryptionType=''),\n",
       " MountInfo(mountPoint='/databricks-results', source='databricks-results', encryptionType=''),\n",
       " MountInfo(mountPoint='/databricks/mlflow-registry', source='databricks/mlflow-registry', encryptionType=''),\n",
       " MountInfo(mountPoint='/Volume', source='DbfsReserved', encryptionType=''),\n",
       " MountInfo(mountPoint='/volumes', source='DbfsReserved', encryptionType=''),\n",
       " MountInfo(mountPoint='/mnt/data/', source='wasbs://rag-files@rahulsadls.blob.core.windows.net', encryptionType=''),\n",
       " MountInfo(mountPoint='/', source='DatabricksRoot', encryptionType=''),\n",
       " MountInfo(mountPoint='/volume', source='DbfsReserved', encryptionType='')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.mounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a25a9030-2cdc-4ab7-84d3-b140de4ebee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: transformers in /databricks/python3/lib/python3.10/site-packages (4.36.1)\nCollecting pypdf\n  Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 kB 6.4 MB/s eta 0:00:00\nRequirement already satisfied: sentence-transformers in /databricks/python3/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /databricks/python3/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.10/site-packages (from transformers) (2022.7.9)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.10/site-packages (from transformers) (23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /databricks/python3/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /databricks/python3/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: typing_extensions>=4.0 in /databricks/python3/lib/python3.10/site-packages (from pypdf) (4.4.0)\nRequirement already satisfied: torch>=1.6.0 in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers) (2.0.1+cpu)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers) (1.10.0)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers) (1.1.1)\nRequirement already satisfied: nltk in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers) (3.7)\nRequirement already satisfied: sentencepiece in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers) (0.1.99)\nRequirement already satisfied: torchvision in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers) (0.15.2+cpu)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\nRequirement already satisfied: sympy in /databricks/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.8.4)\nRequirement already satisfied: click in /databricks/python3/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.0.4)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /databricks/python3/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\nRequirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.2.1)\nInstalling collected packages: pypdf\nSuccessfully installed pypdf-5.1.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers pypdf sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "647aad32-da88-46f8-8a29-2ffa6c553def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff13188-fc2c-4c8a-903a-10662ea5b61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.<storage_account>.blob.core.windows.net\",\n",
    "    \"<storage_account_access_token>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df59ae6c-c5ab-4544-b7cd-709c55884913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='dbfs:/mnt/data/Machine learning - Wikipedia.pdf', name='Machine learning - Wikipedia.pdf', size=2318840, modificationTime=1733143863000)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/mnt/data/Machine learning - Wikipedia.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53c0e7ad-151c-4abd-ab23-cf9e148aaf82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c856b2-0644-44e5-903b-719ed2cc4cc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning\nMachine learning (ML) is a field of study in artificial intelligence concerned with the development\nand study of statistical algorithms that can learn from data and generalize to unseen data, and thus\nperform tasks without explicit instructions.[1] Advances in the field of deep learning have allowed\nneural networks to surpass many previous approaches in performance.[2]\nML finds application in many fields, including natural language processing, computer vision, speech\nrecognition, email filtering, agriculture, and medicine.[3][4] The application of ML to business\nproblems is known as predictive analytics.\nStatistics and mathematical optimization (mathematical programming) methods comprise the\nfoundations of machine learning. Data mining is a related field of study, focusing on exploratory data\nanalysis (EDA) via unsupervised learning.[6][7]\nFrom a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework\nfor describing machine learning.\nThe term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in\nthe field of computer gaming and artificial intelligence.[8][9] The synonym self-teaching computers\nwas also used in this time period.[10][11]\nAlthough the earliest machine learning model was introduced in the 1950s when Arthur Samuel\ninvented a program that calculated the winning chance in checkers for each side, the history of\nmachine learning roots back to decades of human desire and effort to study human cognitive\nprocesses.[12] In 1949, Canadian psychologist Donald Hebb published the book The Organization of\nBehavior, in which he introduced a theoretical neural structure formed by certain interactions among\nnerve cells.[13] Hebb's model of neurons interacting with one another set a groundwork for how AIs\nand machine learning algorithms work under nodes, or artificial neurons used by computers to\ncommunicate data.[12] Other researchers who have studied human cognitive systems contributed to\nthe modern machine learning technologies as well, including logician Walter Pitts and Warren\nMcCulloch, who proposed the early mathematical models of neural networks to come up with\nalgorithms that mirror human thought processes.[12]\nBy the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron,\nhad been developed by Raytheon Company to analyze sonar signals, electrocardiograms, and speech\npatterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human\noperator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to reevaluate\nincorrect decisions.[14] A representative book on research into machine learning during the 1960s was\nHistoryMachine learning as subfield of\nAI[21]\nNilsson's book on Learning Machines, dealing mostly with machine learning for pattern\nclassification.[15] Interest related to pattern recognition continued into the 1970s, as described by\nDuda and Hart in 1973.[16] In 1981 a report was given on using teaching strategies so that an artificial\nneural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a\ncomputer terminal.[17]\nTom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the\nmachine learning field: \"A computer program is said to learn from experience E with respect to some\nclass of tasks T and performance measure P if its performance at tasks in T, as measured by P,\nimproves with experience E.\"[18] This definition of the tasks in which machine learning is concerned\noffers a fundamentally operational definition rather than defining the field in cognitive terms. This\nfollows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the\nquestion \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking\nentities) can do?\".[19]\nModern-day machine learning has two objectives. One is to classify data based on models which have\nbeen developed; the other purpose is to make predictions for future outcomes based on these models.\nA hypothetical algorithm specific to classifying data may use computer vision of moles coupled with\nsupervised learning in order to train it to classify the cancerous moles. A machine learning algorithm\nfor stock trading may inform the trader of future potential predictions.[20]\nAs a scientific endeavor, machine learning grew out of the quest\nfor artificial intelligence (AI). In the early days of AI as an\nacademic discipline, some researchers were interested in having\nmachines learn from data. They attempted to approach the\nproblem with various symbolic methods, as well as what were then\ntermed \"neural networks\"; these were mostly perceptrons and\nother models that were later found to be reinventions of the\ngeneralized linear models of statistics.[22] Probabilistic reasoning\nwas also employed, especially in automated medical\ndiagnosis.[23]: 488 \nHowever, an increasing emphasis on the logical, knowledge-based\napproach caused a rift between AI and machine learning.\nProbabilistic systems were plagued by theoretical and practical\nproblems of data acquisition and representation.[23]: 488  By 1980,\nexpert systems had come to dominate AI, and statistics was out of favor.[24] Work on\nsymbolic/knowledge-based learning did continue within AI, leading to inductive logic\nprogramming(ILP), but the more statistical line of research was now outside the field of AI proper, in\npattern recognition and information retrieval.[23]: 708–710, 755  Neural networks research had been\nabandoned by AI and computer science around the same time. This line, too, was continued outside\nRelationships to other fields\nArtificial intelligencethe AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield,\nDavid Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the\nreinvention of backpropagation.[23]: 25 \nMachine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s.\nThe field changed its goal from achieving artificial intelligence to tackling solvable problems of a\npractical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and\ntoward methods and models borrowed from statistics, fuzzy logic, and probability theory.[24]\nThere is a close connection between machine learning and compression. A system that predicts the\nposterior probabilities of a sequence given its entire history can be used for optimal data compression\n(by using arithmetic coding on the output distribution). Conversely, an optimal compressor can be\nused for prediction (by finding the symbol that compresses best, given the previous history). This\nequivalence has been used as a justification for using data compression as a benchmark for \"general\nintelligence\".[25][26][27]\nAn alternative view can show compression algorithms implicitly map strings into implicit feature\nspace vectors, and compression-based similarity measures compute similarity within these feature\nspaces. For each compressor C(.) we define an associated vector space ℵ , such that C(.) maps an input\nstring x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces\nunderlying all compression algorithms is precluded by space; instead, feature vectors chooses to\nexamine three representative lossless compression methods, LZW, LZ77, and PPM.[28]\nAccording to AIXI theory, a connection more directly explained in Hutter Prize, the best possible\ncompression of x is the smallest possible software that generates x. For example, in that model, a zip\nfile's compressed size includes both the zip file and the unzipping software, since you can not unzip it\nwithout both, but there may be an even smaller combined form.\nExamples of AI-powered audio/video compression software include NVIDIA Maxine, AIVC.[29]\nExamples of software that can perform AI-powered image compression include OpenCV, TensorFlow,\nMATLAB's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression.[30]\nIn unsupervised machine learning, k-means clustering can be utilized to compress data by grouping\nsimilar data points into clusters. This technique simplifies handling extensive datasets that lack\npredefined labels and finds widespread use in fields such as image compression.[31]\nData compression aims to reduce the size of data files, enhancing storage efficiency and speeding up\ndata transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to\npartition a dataset into a specified number of clusters, k, each represented by the centroid of its\npoints. This process condenses extensive datasets into a more compact set of representative points.\nParticularly beneficial in image and signal processing, k-means clustering aids in data reduction by\nreplacing groups of data points with their centroids, thereby preserving the core information of the\noriginal data while significantly decreasing the required storage space.[32]\nData compressionLarge language models (LLMs) are also capable of lossless data compression, as demonstrated by\nDeepMind's research with the Chinchilla 70B model. Developed by DeepMind, Chinchilla 70B\neffectively compressed data, outperforming conventional methods such as Portable Network Graphics\n(PNG) for images and Free Lossless Audio Codec (FLAC) for audio. It achieved compression of image\nand audio data to 43.4% and 16.4% of their original sizes, respectively.[33]\nMachine learning and data mining often employ the same methods and overlap significantly, but\nwhile machine learning focuses on prediction, based on known properties learned from the training\ndata, data mining focuses on the discovery of (previously) unknown properties in the data (this is the\nanalysis step of knowledge discovery in databases). Data mining uses many machine learning\nmethods, but with different goals; on the other hand, machine learning also employs data mining\nmethods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of\nthe confusion between these two research communities (which do often have separate conferences\nand separate journals, ECML PKDD being a major exception) comes from the basic assumptions they\nwork with: in machine learning, performance is usually evaluated with respect to the ability to\nreproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the\ndiscovery of previously unknown knowledge. Evaluated with respect to known knowledge, an\nuninformed (unsupervised) method will easily be outperformed by other supervised methods, while in\na typical KDD task, supervised methods cannot be used due to the unavailability of training data.\nMachine learning also has intimate ties to optimization: Many learning problems are formulated as\nminimization of some loss function on a training set of examples. Loss functions express the\ndiscrepancy between the predictions of the model being trained and the actual problem instances (for\nexample, in classification, one wants to assign a label to instances, and models are trained to correctly\npredict the preassigned labels of a set of examples).[34]\nCharacterizing the generalization of various learning algorithms is an active topic of current research,\nespecially for deep learning algorithms.\nMachine learning and statistics are closely related fields in terms of methods, but distinct in their\nprincipal goal: statistics draws population inferences from a sample, while machine learning finds\ngeneralizable predictive patterns.[35] According to Michael I. Jordan, the ideas of machine learning,\nfrom methodological principles to theoretical tools, have had a long pre-history in statistics.[36] He\nalso suggested the term data science as a placeholder to call the overall field.[36]\nConventional statistical analyses require the a priori selection of a model most suitable for the study\ndata set. In addition, only significant or theoretically relevant variables based on previous experience\nare included for analysis. In contrast, machine learning is not built on a pre-structured model; rather,\nthe data shape the model by detecting underlying patterns. The more variables (input) used to train\nthe model, the more accurate the ultimate model will be.[37]\nData mining\nGeneralization\nStatisticsLeo Breiman distinguished two statistical modeling paradigms: data model and algorithmic\nmodel,[38] wherein \"algorithmic model\" means more or less the machine learning algorithms like\nRandom Forest.\nSome statisticians have adopted methods from machine learning, leading to a combined field that\nthey call statistical learning.[39]\nAnalytical and computational techniques derived from deep-rooted physics of disordered systems can\nbe extended to large-scale problems, including machine learning, e.g., to analyze the weight space of\ndeep neural networks.[40] Statistical physics is thus finding applications in the area of medical\ndiagnostics.[41]\nA core objective of a learner is to generalize from its experience.[5][42] Generalization in this context is\nthe ability of a learning machine to perform accurately on new, unseen examples/tasks after having\nexperienced a learning data set. The training examples come from some generally unknown\nprobability distribution (considered representative of the space of occurrences) and the learner has to\nbuild a general model about this space that enables it to produce sufficiently accurate predictions in\nnew cases.\nThe computational analysis of machine learning algorithms and their performance is a branch of\ntheoretical computer science known as computational learning theory via the Probably Approximately\nCorrect Learning (PAC) model. Because training sets are finite and the future is uncertain, learning\ntheory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic\nbounds on the performance are quite common. The bias–variance decomposition is one way to\nquantify generalization error.\nFor the best performance in the context of generalization, the complexity of the hypothesis should\nmatch the complexity of the function underlying the data. If the hypothesis is less complex than the\nfunction, then the model has under fitted the data. If the complexity of the model is increased in\nresponse, then the training error decreases. But if the hypothesis is too complex, then the model is\nsubject to overfitting and generalization will be poorer.[43]\nIn addition to performance bounds, learning theorists study the time complexity and feasibility of\nlearning. In computational learning theory, a computation is considered feasible if it can be done in\npolynomial time. There are two kinds of time complexity results: Positive results show that a certain\nclass of functions can be learned in polynomial time. Negative results show that certain classes cannot\nbe learned in polynomial time.\nStatistical physics\nTheory\nApproachesIn supervised learning, the training data is\nlabeled with the expected answers, while in\nunsupervised learning, the model identifies\npatterns or structures in unlabeled data.\nA support-vector machine is a\nsupervised learning model that\ndivides the data into regions\nseparated by a linear boundary.\nHere, the linear boundary divides\nthe black circles from the white.\nMachine learning approaches are traditionally divided\ninto three broad categories, which correspond to\nlearning paradigms, depending on the nature of the\n\"signal\" or \"feedback\" available to the learning system:\nSupervised learning: The computer is presented with\nexample inputs and their desired outputs, given by a\n\"teacher\", and the goal is to learn a general rule that\nmaps inputs to outputs.\nUnsupervised learning: No labels are given to the\nlearning algorithm, leaving it on its own to find\nstructure in its input. Unsupervised learning can be a\ngoal in itself (discovering hidden patterns in data) or\na means towards an end (feature learning).\nReinforcement learning: A computer program interacts with a dynamic environment in which it\nmust perform a certain goal (such as driving a vehicle or playing a game against an opponent). As\nit navigates its problem space, the program is provided feedback that's analogous to rewards,\nwhich it tries to maximize.[5]\nAlthough each algorithm has advantages and limitations, no single algorithm works for all\nproblems.[44][45][46]\nSupervised learning algorithms build a mathematical model of a\nset of data that contains both the inputs and the desired\noutputs.[47] The data, known as training data, consists of a set of\ntraining examples. Each training example has one or more inputs\nand the desired output, also known as a supervisory signal. In the\nmathematical model, each training example is represented by an\narray or vector, sometimes called a feature vector, and the training\ndata is represented by a matrix. Through iterative optimization of\nan objective function, supervised learning algorithms learn a\nfunction that can be used to predict the output associated with\nnew inputs.[48] An optimal function allows the algorithm to\ncorrectly determine the output for inputs that were not a part of\nthe training data. An algorithm that improves the accuracy of its\noutputs or predictions over time is said to have learned to perform\nthat task.[18]\nTypes of supervised-learning algorithms include active learning,\nclassification and regression.[49] Classification algorithms are used\nwhen the outputs are restricted to a limited set of values, and\nregression algorithms are used when the outputs may have any numerical value within a range. As an\nexample, for a classification algorithm that filters emails, the input would be an incoming email, and\nthe output would be the name of the folder in which to file the email. Examples of regression would be\npredicting the height of a person, or the future temperature. [50]\nSupervised learningSimilarity learning is an area of supervised machine learning closely related to regression and\nclassification, but the goal is to learn from examples using a similarity function that measures how\nsimilar or related two objects are. It has applications in ranking, recommendation systems, visual\nidentity tracking, face verification, and speaker verification.\nUnsupervised learning algorithms find structures in data that has not been labeled, classified or\ncategorized. Instead of responding to feedback, unsupervised learning algorithms identify\ncommonalities in the data and react based on the presence or absence of such commonalities in each\nnew piece of data. Central applications of unsupervised machine learning include clustering,\ndimensionality reduction,[7] and density estimation.[51]\nCluster analysis is the assignment of a set of observations into subsets (called clusters) so that\nobservations within the same cluster are similar according to one or more predesignated criteria,\nwhile observations drawn from different clusters are dissimilar. Different clustering techniques make\ndifferent assumptions on the structure of the data, often defined by some similarity metric and\nevaluated, for example, by internal compactness, or the similarity between members of the same\ncluster, and separation, the difference between clusters. Other methods are based on estimated\ndensity and graph connectivity.\nA special type of unsupervised learning called, self-supervised learning involves training a model by\ngenerating the supervisory signal from the data itself.[52][53]\nSemi-supervised learning falls between unsupervised learning (without any labeled training data) and\nsupervised learning (with completely labeled training data). Some of the training examples are\nmissing training labels, yet many machine-learning researchers have found that unlabeled data, when\nused in conjunction with a small amount of labeled data, can produce a considerable improvement in\nlearning accuracy.\nIn weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these\nlabels are often cheaper to obtain, resulting in larger effective training sets.[54]\nReinforcement learning is an area of machine learning concerned with how software agents ought to\ntake actions in an environment so as to maximize some notion of cumulative reward. Due to its\ngenerality, the field is studied in many other disciplines, such as game theory, control theory,\noperations research, information theory, simulation-based optimization, multi-agent systems, swarm\nintelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically\nrepresented as a Markov decision process (MDP). Many reinforcements learning algorithms use\ndynamic programming techniques.[55] Reinforcement learning algorithms do not assume knowledge\nUnsupervised learning\nSemi-supervised learning\nReinforcement learningof an exact mathematical model of the MDP and are used when exact\nmodels are infeasible. Reinforcement learning algorithms are used in\nautonomous vehicles or in learning to play a game against a human\nopponent.\nDimensionality reduction is a process of reducing the number of\nrandom variables under consideration by obtaining a set of principal\nvariables.[56] In other words, it is a process of reducing the dimension\nof the feature set, also called the \"number of features\". Most of the\ndimensionality reduction techniques can be considered as either feature elimination or extraction.\nOne of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA\ninvolves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). The manifold\nhypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many\ndimensionality reduction techniques make this assumption, leading to the area of manifold learning\nand manifold regularization.\nOther approaches have been developed which do not fit neatly into this three-fold categorization, and\nsometimes more than one is used by the same machine learning system. For example, topic modeling,\nmeta-learning.[57]\nSelf-learning, as a machine learning paradigm was introduced in 1982 along with a neural network\ncapable of self-learning, named crossbar adaptive array (CAA).[58] It is learning with no external\nrewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar\nfashion, both decisions about actions and emotions (feelings) about consequence situations. The\nsystem is driven by the interaction between cognition and emotion.[59] The self-learning algorithm\nupdates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine\nlearning routine:\n1. in situation s perform action a\n2. receive a consequence situation s'\n3. compute emotion of being in the consequence situation v(s')\n4. update crossbar memory w'(a,s) = w(a,s) + v(s')\nIt is a system with only one input, situation, and only one output, action (or behavior) a. There is\nneither a separate reinforcement input nor an advice input from the environment. The\nbackpropagated value (secondary reinforcement) is the emotion toward the consequence situation.\nThe CAA exists in two environments, one is the behavioral environment where it behaves, and the\nother is the genetic environment, wherefrom it initially and only once receives initial emotions about\nDimensionality reduction\nOther types\nSelf-learningsituations to be encountered in the behavioral environment. After receiving the genome (species)\nvector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that\ncontains both desirable and undesirable situations.[60]\nSeveral learning algorithms aim at discovering better representations of the inputs provided during\ntraining.[61] Classic examples include principal component analysis and cluster analysis. Feature\nlearning algorithms, also called representation learning algorithms, often attempt to preserve the\ninformation in their input but also transform it in a way that makes it useful, often as a pre-processing\nstep before performing classification or predictions. This technique allows reconstruction of the\ninputs coming from the unknown data-generating distribution, while not being necessarily faithful to\nconfigurations that are implausible under that distribution. This replaces manual feature engineering,\nand allows a machine to both learn the features and use them to perform a specific task.\nFeature learning can be either supervised or unsupervised. In supervised feature learning, features are\nlearned using labeled input data. Examples include artificial neural networks, multilayer perceptrons,\nand supervised dictionary learning. In unsupervised feature learning, features are learned with\nunlabeled input data. Examples include dictionary learning, independent component analysis,\nautoencoders, matrix factorization[62] and various forms of clustering.[63][64][65]\nManifold learning algorithms attempt to do \n\n*** WARNING: max output size exceeded, skipping output. ***\n\n.\n119. \"Why Uber's self-driving car killed a pedestrian\" (https://www.economist.com/the-economist-explai\nns/2018/05/29/why-ubers-self-driving-car-killed-a-pedestrian). The Economist. Archived (https://w\neb.archive.org/web/20180821031818/https://www.economist.com/the-economist-explains/2018/0\n5/29/why-ubers-self-driving-car-killed-a-pedestrian) from the original on 2018-08-21. Retrieved\n2018-08-20.\n120. \"IBM's Watson recommended 'unsafe and incorrect' cancer treatments – STAT\" (https://www.statn\news.com/2018/07/25/ibm-watson-recommended-unsafe-incorrect-treatments/). STAT. 2018-07-25.\nArchived (https://web.archive.org/web/20180821062616/https://www.statnews.com/2018/07/25/ib\nm-watson-recommended-unsafe-incorrect-treatments/) from the original on 2018-08-21. Retrieved\n2018-08-21.\n121. Hernandez, Daniela; Greenwald, Ted (2018-08-11). \"IBM Has a Watson Dilemma\" (https://www.ws\nj.com/articles/ibm-bet-billions-that-watson-could-improve-cancer-treatment-it-hasnt-worked-15339\n61147). The Wall Street Journal. ISSN 0099-9660 (https://search.worldcat.org/issn/0099-9660).\nArchived (https://web.archive.org/web/20180821031906/https://www.wsj.com/articles/ibm-bet-billio\nns-that-watson-could-improve-cancer-treatment-it-hasnt-worked-1533961147) from the original on\n2018-08-21. Retrieved 2018-08-21.\n122. Allyn, Bobby (Feb 27, 2023). \"How Microsoft's experiment in artificial intelligence tech backfired\"\n(https://www.npr.org/2023/02/27/1159630243/how-microsofts-experiment-in-artificial-intelligence-t\nech-backfired). National Public Radio. Archived (https://web.archive.org/web/20231208234056/htt\nps://www.npr.org/2023/02/27/1159630243/how-microsofts-experiment-in-artificial-intelligence-tech-\nbackfired) from the original on December 8, 2023. Retrieved Dec 8, 2023.\n123. Reddy, Shivani M.; Patel, Sheila; Weyrich, Meghan; Fenton, Joshua; Viswanathan, Meera (2020).\n\"Comparison of a traditional systematic review approach with review-of-reviews and semi-\nautomation as strategies to update the evidence\" (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC\n7574591). Systematic Reviews. 9 (1): 243. doi:10.1186/s13643-020-01450-2 (https://doi.org/10.11\n86%2Fs13643-020-01450-2). ISSN 2046-4053 (https://search.worldcat.org/issn/2046-4053).\nPMC 7574591 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7574591). PMID 33076975 (https://\npubmed.ncbi.nlm.nih.gov/33076975).\n124. Rudin, Cynthia (2019). \"Stop explaining black box machine learning models for high stakes\ndecisions and use interpretable models instead\" (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9\n122117). Nature Machine Intelligence. 1 (5): 206–215. doi:10.1038/s42256-019-0048-x (https://do\ni.org/10.1038%2Fs42256-019-0048-x). PMC 9122117 (https://www.ncbi.nlm.nih.gov/pmc/articles/\nPMC9122117). PMID 35603010 (https://pubmed.ncbi.nlm.nih.gov/35603010).\n125. Hu, Tongxi; Zhang, Xuesong; Bohrer, Gil; Liu, Yanlan; Zhou, Yuyu; Martin, Jay; LI, Yang; Zhao,\nKaiguang (2023). \"Crop yield prediction via explainable AI and interpretable machine learning:\nDangers of black box models for evaluating climate change impacts on crop yield\" (https://doi.org/\n10.1016%2Fj.agrformet.2023.109458). Agricultural and Forest Meteorology. 336: 109458.\ndoi:10.1016/j.agrformet.2023.109458 (https://doi.org/10.1016%2Fj.agrformet.2023.109458).\nS2CID 258552400 (https://api.semanticscholar.org/CorpusID:258552400).\n126. Domingos 2015, Chapter 6, Chapter 7.\n127. Domingos 2015, p. 286.128. \"Single pixel change fools AI programs\" (https://www.bbc.com/news/technology-41845878). BBC\nNews. 3 November 2017. Archived (https://web.archive.org/web/20180322011306/http://www.bbc.\ncom/news/technology-41845878) from the original on 22 March 2018. Retrieved 12 March 2018.\n129. \"AI Has a Hallucination Problem That's Proving Tough to Fix\" (https://www.wired.com/story/ai-has-\na-hallucination-problem-thats-proving-tough-to-fix/). WIRED. 2018. Archived (https://web.archive.o\nrg/web/20180312024533/https://www.wired.com/story/ai-has-a-hallucination-problem-thats-provin\ng-tough-to-fix/) from the original on 12 March 2018. Retrieved 12 March 2018.\n130. Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; Vladu, A. (4 September 2019). \"Towards deep\nlearning models resistant to adversarial attacks\". arXiv:1706.06083 (https://arxiv.org/abs/1706.060\n83) [stat.ML (https://arxiv.org/archive/stat.ML)].\n131. \"Adversarial Machine Learning – CLTC UC Berkeley Center for Long-Term Cybersecurity\" (https://\ncltc.berkeley.edu/aml/). CLTC. Archived (https://web.archive.org/web/20220517045352/https://cltc.\nberkeley.edu/aml/) from the original on 2022-05-17. Retrieved 2022-05-25.\n132. \"Machine-learning models vulnerable to undetectable backdoors\" (https://www.theregister.com/20\n22/04/21/machine_learning_models_backdoors/). The Register. Archived (https://web.archive.org/\nweb/20220513171215/https://www.theregister.com/2022/04/21/machine_learning_models_backdo\nors/) from the original on 13 May 2022. Retrieved 13 May 2022.\n133. \"Undetectable Backdoors Plantable In Any Machine-Learning Algorithm\" (https://spectrum.ieee.or\ng/machine-learningbackdoor). IEEE Spectrum. 10 May 2022. Archived (https://web.archive.org/we\nb/20220511152052/https://spectrum.ieee.org/machine-learningbackdoor) from the original on 11\nMay 2022. Retrieved 13 May 2022.\n134. Goldwasser, Shafi; Kim, Michael P.; Vaikuntanathan, Vinod; Zamir, Or (14 April 2022). \"Planting\nUndetectable Backdoors in Machine Learning Models\". arXiv:2204.06974 (https://arxiv.org/abs/22\n04.06974) [cs.LG (https://arxiv.org/archive/cs.LG)].\n135. Kohavi, Ron (1995). \"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and\nModel Selection\" (https://ai.stanford.edu/~ronnyk/accEst.pdf) (PDF). International Joint\nConference on Artificial Intelligence. Archived (https://web.archive.org/web/20180712102706/htt\np://web.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf) (PDF) from the original on 2018-\n07-12. Retrieved 2023-03-26.\n136. Catal, Cagatay (2012). \"Performance Evaluation Metrics for Software Fault Prediction Studies\" (htt\np://www.uni-obuda.hu/journal/Catal_36.pdf) (PDF). Acta Polytechnica Hungarica. 9 (4). Retrieved\n2 October 2016.\n137. Cite error: The named reference Ethics of artificial intelligence Müller-2020 was\ninvoked but never defined (see the help page).\n138. Garcia, Megan (2016). \"Racist in the Machine\". World Policy Journal. 33 (4): 111–117.\ndoi:10.1215/07402775-3813015 (https://doi.org/10.1215%2F07402775-3813015). ISSN 0740-\n2775 (https://search.worldcat.org/issn/0740-2775). S2CID 151595343 (https://api.semanticschola\nr.org/CorpusID:151595343).\n139. Bostrom, Nick (2011). \"The Ethics of Artificial Intelligence\" (https://web.archive.org/web/20160304\n015020/http://www.nickbostrom.com/ethics/artificial-intelligence.pdf) (PDF). Archived from the\noriginal (http://www.nickbostrom.com/ethics/artificial-intelligence.pdf) (PDF) on 4 March 2016.\nRetrieved 11 April 2016.\n140. Edionwe, Tolulope. \"The fight against racist algorithms\" (https://theoutline.com/post/1571/the-fight-\nagainst-racist-algorithms). The Outline. Archived (https://web.archive.org/web/20171117174504/htt\nps://theoutline.com/post/1571/the-fight-against-racist-algorithms) from the original on 17\nNovember 2017. Retrieved 17 November 2017.\n141. Jeffries, Adrianne. \"Machine learning is racist because the internet is racist\" (https://theoutline.co\nm/post/1439/machine-learning-is-racist-because-the-internet-is-racist). The Outline. Archived (http\ns://web.archive.org/web/20171117174503/https://theoutline.com/post/1439/machine-learning-is-ra\ncist-because-the-internet-is-racist) from the original on 17 November 2017. Retrieved\n17 November 2017.142. Silva, Selena; Kenney, Martin (2018). \"Algorithms, Platforms, and Ethnic Bias: An Integrative\nEssay\" (https://brie.berkeley.edu/sites/default/files/brie_wp_2018-3.pdf) (PDF). Phylon. 55 (1 & 2):\n9–37. ISSN 0031-8906 (https://search.worldcat.org/issn/0031-8906). JSTOR 26545017 (https://w\nww.jstor.org/stable/26545017). Archived (https://web.archive.org/web/20240127200319/https://bri\ne.berkeley.edu/sites/default/files/brie_wp_2018-3.pdf) (PDF) from the original on Jan 27, 2024.\n143. Wong, Carissa (2023-03-30). \"AI 'fairness' research held back by lack of diversity\" (https://www.na\nture.com/articles/d41586-023-00935-z). Nature. doi:10.1038/d41586-023-00935-z (https://doi.org/\n10.1038%2Fd41586-023-00935-z). PMID 36997714 (https://pubmed.ncbi.nlm.nih.gov/36997714).\nS2CID 257857012 (https://api.semanticscholar.org/CorpusID:257857012). Archived (https://web.a\nrchive.org/web/20230412120505/https://www.nature.com/articles/d41586-023-00935-z) from the\noriginal on 2023-04-12. Retrieved 2023-12-09.\n144. Zhang, Jack Clark. \"Artificial Intelligence Index Report 2021\" (https://aiindex.stanford.edu/wp-cont\nent/uploads/2021/11/2021-AI-Index-Report_Master.pdf) (PDF). Stanford Institute for Human-\nCentered Artificial Intelligence. Archived (https://web.archive.org/web/20240519121545/https://aiin\ndex.stanford.edu/wp-content/uploads/2021/11/2021-AI-Index-Report_Master.pdf) (PDF) from the\noriginal on 2024-05-19. Retrieved 2023-12-09.\n145. Caliskan, Aylin; Bryson, Joanna J.; Narayanan, Arvind (2017-04-14). \"Semantics derived\nautomatically from language corpora contain human-like biases\". Science. 356 (6334): 183–186.\narXiv:1608.07187 (https://arxiv.org/abs/1608.07187). Bibcode:2017Sci...356..183C (https://ui.adsa\nbs.harvard.edu/abs/2017Sci...356..183C). doi:10.1126/science.aal4230 (https://doi.org/10.1126%2\nFscience.aal4230). ISSN 0036-8075 (https://search.worldcat.org/issn/0036-8075).\nPMID 28408601 (https://pubmed.ncbi.nlm.nih.gov/28408601). S2CID 23163324 (https://api.sema\nnticscholar.org/CorpusID:23163324).\n146. Wang, Xinan; Dasgupta, Sanjoy (2016), Lee, D. D.; Sugiyama, M.; Luxburg, U. V.; Guyon, I.\n(eds.), \"An algorithm for L1 nearest neighbor search via monotonic embedding\" (http://papers.nip\ns.cc/paper/6227-an-algorithm-for-l1-nearest-neighbor-search-via-monotonic-embedding.pdf)\n(PDF), Advances in Neural Information Processing Systems 29, Curran Associates, Inc., pp. 983–\n991, archived (https://web.archive.org/web/20170407051313/http://papers.nips.cc/paper/6227-an-\nalgorithm-for-l1-nearest-neighbor-search-via-monotonic-embedding.pdf) (PDF) from the original\non 2017-04-07, retrieved 2018-08-20\n147. M.O.R. Prates; P.H.C. Avelar; L.C. Lamb (11 Mar 2019). \"Assessing Gender Bias in Machine\nTranslation – A Case Study with Google Translate\". arXiv:1809.02208 (https://arxiv.org/abs/1809.0\n2208) [cs.CY (https://arxiv.org/archive/cs.CY)].\n148. Narayanan, Arvind (August 24, 2016). \"Language necessarily contains human biases, and so will\nmachines trained on language corpora\" (https://freedom-to-tinker.com/2016/08/24/language-neces\nsarily-contains-human-biases-and-so-will-machines-trained-on-language-corpora/). Freedom to\nTinker. Archived (https://web.archive.org/web/20180625021555/https://freedom-to-tinker.com/201\n6/08/24/language-necessarily-contains-human-biases-and-so-will-machines-trained-on-language-\ncorpora/) from the original on June 25, 2018. Retrieved November 19, 2016.\n149. Metz, Rachel (March 24, 2016). \"Why Microsoft Accidentally Unleashed a Neo-Nazi Sexbot\" (http\ns://www.technologyreview.com/s/601111/why-microsoft-accidentally-unleashed-a-neo-nazi-sexbo\nt/). MIT Technology Review. Archived (https://web.archive.org/web/20181109023754/https://www.t\nechnologyreview.com/s/601111/why-microsoft-accidentally-unleashed-a-neo-nazi-sexbot/) from\nthe original on 2018-11-09. Retrieved 2018-08-20.\n150. Vincent, James (Jan 12, 2018). \"Google 'fixed' its racist algorithm by removing gorillas from its\nimage-labeling tech\" (https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-\nrecognition-algorithm-ai). The Verge. Archived (https://web.archive.org/web/20180821031830/http\ns://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai)\nfrom the original on 2018-08-21. Retrieved 2018-08-20.151. Crawford, Kate (25 June 2016). \"Opinion | Artificial Intelligence's White Guy Problem\" (https://ww\nw.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html). New\nYork Times. Archived (https://web.archive.org/web/20210114220619/https://www.nytimes.com/201\n6/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html) from the original on 2021-\n01-14. Retrieved 2018-08-20.\n152. Simonite, Tom (March 30, 2017). \"Microsoft: AI Isn't Yet Adaptable Enough to Help Businesses\" (ht\ntps://www.technologyreview.com/s/603944/microsoft-ai-isnt-yet-adaptable-enough-to-help-busines\nses/). MIT Technology Review. Archived (https://web.archive.org/web/20181109022820/https://ww\nw.technologyreview.com/s/603944/microsoft-ai-isnt-yet-adaptable-enough-to-help-businesses/)\nfrom the original on 2018-11-09. Retrieved 2018-08-20.\n153. Hempel, Jessi (2018-11-13). \"Fei-Fei Li's Quest to Make Machines Better for Humanity\" (https://w\nww.wired.com/story/fei-fei-li-artificial-intelligence-humanity/). Wired. ISSN 1059-1028 (https://searc\nh.worldcat.org/issn/1059-1028). Archived (https://web.archive.org/web/20201214095220/https://w\nww.wired.com/story/fei-fei-li-artificial-intelligence-humanity/) from the original on 2020-12-14.\nRetrieved 2019-02-17.\n154. Char, D. S.; Shah, N. H.; Magnus, D. (2018). \"Implementing Machine Learning in Health Care—\nAddressing Ethical Challenges\" (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5962261). New\nEngland Journal of Medicine. 378 (11): 981–983. doi:10.1056/nejmp1714229 (https://doi.org/10.10\n56%2Fnejmp1714229). PMC 5962261 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5962261).\nPMID 29539284 (https://pubmed.ncbi.nlm.nih.gov/29539284).\n155. Research, AI (23 October 2015). \"Deep Neural Networks for Acoustic Modeling in Speech\nRecognition\" (http://airesearch.com/ai-research-papers/deep-neural-networks-for-acoustic-modeli\nng-in-speech-recognition/). airesearch.com. Archived (https://web.archive.org/web/201602010338\n01/http://airesearch.com/ai-research-papers/deep-neural-networks-for-acoustic-modeling-in-speec\nh-recognition/) from the original on 1 February 2016. Retrieved 23 October 2015.\n156. \"GPUs Continue to Dominate the AI Accelerator Market for Now\" (https://www.informationweek.co\nm/big-data/ai-machine-learning/gpus-continue-to-dominate-the-ai-accelerator-market-for-now/a/d-i\nd/1336475). InformationWeek. December 2019. Archived (https://web.archive.org/web/202006100\n94310/https://www.informationweek.com/big-data/ai-machine-learning/gpus-continue-to-dominate-\nthe-ai-accelerator-market-for-now/a/d-id/1336475) from the original on 10 June 2020. Retrieved\n11 June 2020.\n157. Ray, Tiernan (2019). \"AI is changing the entire nature of compute\" (https://www.zdnet.com/article/\nai-is-changing-the-entire-nature-of-compute/). ZDNet. Archived (https://web.archive.org/web/2020\n0525144635/https://www.zdnet.com/article/ai-is-changing-the-entire-nature-of-compute/) from the\noriginal on 25 May 2020. Retrieved 11 June 2020.\n158. \"AI and Compute\" (https://openai.com/blog/ai-and-compute/). OpenAI. 16 May 2018. Archived (htt\nps://web.archive.org/web/20200617200602/https://openai.com/blog/ai-and-compute/) from the\noriginal on 17 June 2020. Retrieved 11 June 2020.\n159. \"What is neuromorphic computing? Everything you need to know about how it is changing the\nfuture of computing\" (https://www.zdnet.com/article/what-is-neuromorphic-computing-everything-y\nou-need-to-know-about-how-it-will-change-the-future-of-computing/). ZDNET. 8 December 2020.\nRetrieved 2024-11-21.\n160. \"Cornell & NTT's Physical Neural Networks: A \"Radical Alternative for Implementing Deep Neural\nNetworks\" That Enables Arbitrary Physical Systems Training\" (https://syncedreview.com/2021/05/\n27/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-28/).\nSynced. 27 May 2021. Archived (https://web.archive.org/web/20211027183428/https://syncedrevie\nw.com/2021/05/27/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-\nat-low-cost-28/) from the original on 27 October 2021. Retrieved 12 October 2021.\n161. \"Nano-spaghetti to solve neural network power consumption\" (https://www.theregister.com/2021/1\n0/05/analogue_neural_network_research/). The Register. 5 October 2021. Archived (https://web.a\nrchive.org/web/20211006150057/https://www.theregister.com/2021/10/05/analogue_neural_netwo\nrk_research/) from the original on 2021-10-06. Retrieved 2021-10-12.162. Fafoutis, Xenofon; Marchegiani, Letizia; Elsts, Atis; Pope, James; Piechocki, Robert; Craddock,\nIan (2018-05-07). \"Extending the battery lifetime of wearable sensors with embedded machine\nlearning\" (https://ieeexplore.ieee.org/document/8355116). 2018 IEEE 4th World Forum on Internet\nof Things (WF-IoT) (https://research-information.bris.ac.uk/en/publications/b8fdb58b-7114-45c6-8\n2e4-4ab239c1327f). pp. 269–274. doi:10.1109/WF-IoT.2018.8355116 (https://doi.org/10.1109%2F\nWF-IoT.2018.8355116). hdl:1983/b8fdb58b-7114-45c6-82e4-4ab239c1327f (https://hdl.handle.net/\n1983%2Fb8fdb58b-7114-45c6-82e4-4ab239c1327f). ISBN 978-1-4673-9944-9. S2CID 19192912\n(https://api.semanticscholar.org/CorpusID:19192912). Archived (https://web.archive.org/web/2022\n0118182543/https://ieeexplore.ieee.org/abstract/document/8355116?casa_token=LCpUeGLS1e8\nAAAAA:2OjuJfNwZBnV2pgDxfnEAC-jbrETv_BpTcX35_aFqN6IULFxu1xbYbVSRpD-zMd4GCUME\nLyG) from the original on 2022-01-18. Retrieved 2022-01-17.\n163. \"A Beginner's Guide To Machine learning For Embedded Systems\" (https://analyticsindiamag.com/\na-beginners-guide-to-machine-learning-for-embedded-systems/). Analytics India Magazine. 2021-\n06-02. Archived (https://web.archive.org/web/20220118182754/https://analyticsindiamag.com/a-be\nginners-guide-to-machine-learning-for-embedded-systems/) from the original on 2022-01-18.\nRetrieved 2022-01-17.\n164. Synced (2022-01-12). \"Google, Purdue & Harvard U's Open-Source Framework for TinyML\nAchieves up to 75x Speedups on FPGAs | Synced\" (https://syncedreview.com/2022/01/12/deepmi\nnd-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-183/).\nsyncedreview.com. Archived (https://web.archive.org/web/20220118182404/https://syncedreview.c\nom/2022/01/12/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-l\now-cost-183/) from the original on 2022-01-18. Retrieved 2022-01-17.\n165. Giri, Davide; Chiu, Kuan-Lin; Di Guglielmo, Giuseppe; Mantovani, Paolo; Carloni, Luca P. (2020-\n06-15). \"ESP4ML: Platform-Based Design of Systems-on-Chip for Embedded Machine Learning\"\n(https://ieeexplore.ieee.org/document/9116317). 2020 Design, Automation & Test in Europe\nConference & Exhibition (DATE). pp. 1049–1054. arXiv:2004.03640 (https://arxiv.org/abs/2004.03\n640). doi:10.23919/DATE48585.2020.9116317 (https://doi.org/10.23919%2FDATE48585.2020.91\n16317). ISBN 978-3-9819263-4-7. S2CID 210928161 (https://api.semanticscholar.org/CorpusID:2\n10928161). Archived (https://web.archive.org/web/20220118182342/https://ieeexplore.ieee.org/ab\nstract/document/9116317?casa_token=5I_Tmgrrbu4AAAAA:v7pDHPEWlRuo2Vk3pU06194PO0-\nW21UOdyZqADrZxrRdPBZDMLwQrjJSAHUhHtzJmLu_VdgW) from the original on 2022-01-18.\nRetrieved 2022-01-17.\n166. Louis, Marcia Sahaya; Azad, Zahra; Delshadtehrani, Leila; Gupta, Suyog; Warden, Pete; Reddi,\nVijay Janapa; Joshi, Ajay (2019). \"Towards Deep Learning using TensorFlow Lite on RISC-V\" (http\ns://edge.seas.harvard.edu/publications/towards-deep-learning-using-tensorflow-lite-risc-v).\nHarvard University. Archived (https://web.archive.org/web/20220117031909/https://edge.seas.har\nvard.edu/publications/towards-deep-learning-using-tensorflow-lite-risc-v) from the original on\n2022-01-17. Retrieved 2022-01-17.\n167. Ibrahim, Ali; Osta, Mario; Alameh, Mohamad; Saleh, Moustafa; Chible, Hussein; Valle, Maurizio\n(2019-01-21). \"Approximate Computing Methods for Embedded Machine Learning\" (https://ieeexpl\nore.ieee.org/document/8617877). 2018 25th IEEE International Conference on Electronics,\nCircuits and Systems (ICECS). pp. 845–848. doi:10.1109/ICECS.2018.8617877 (https://doi.org/1\n0.1109%2FICECS.2018.8617877). ISBN 978-1-5386-9562-3. S2CID 58670712 (https://api.seman\nticscholar.org/CorpusID:58670712). Archived (https://web.archive.org/web/20220117031855/http\ns://ieeexplore.ieee.org/abstract/document/8617877?casa_token=arUW5Oy-tzwAAAAA:I9x6edlfsk\nM6kGNFUN9zAFrjEBv_8kYTz7ERTxtXu9jAqdrYCcDbbwjBdgwXvb6QAH_-0VJJ) from the\noriginal on 2022-01-17. Retrieved 2022-01-17.\n168. \"dblp: TensorFlow Eager: A Multi-Stage, Python-Embedded DSL for Machine Learning\" (https://dbl\np.org/rec/journals/corr/abs-1903-01855.html). dblp.org. Archived (https://web.archive.org/web/202\n20118182335/https://dblp.org/rec/journals/corr/abs-1903-01855.html) from the original on 2022-\n01-18. Retrieved 2022-01-17.169. Branco, Sérgio; Ferreira, André G.; Cabral, Jorge (2019-11-05). \"Machine Learning in Resource-\nScarce Embedded Systems, FPGAs, and End-Devices: A Survey\" (https://doi.org/10.3390%2Fele\nctronics8111289). Electronics. 8 (11): 1289. doi:10.3390/electronics8111289 (https://doi.org/10.339\n0%2Felectronics8111289). hdl:1822/62521 (https://hdl.handle.net/1822%2F62521). ISSN 2079-\n9292 (https://search.worldcat.org/issn/2079-9292).\nDomingos, Pedro (September 22, 2015). The Master Algorithm: How the Quest for the Ultimate\nLearning Machine Will Remake Our World. Basic Books. ISBN 978-0465065707.\nNilsson, Nils (1998). Artificial Intelligence: A New Synthesis (https://archive.org/details/artificialintel\nl0000nils). Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived (https://web.archive.org/web/20\n200726131654/https://archive.org/details/artificialintell0000nils) from the original on 26 July 2020.\nRetrieved 18 November 2019.\nPoole, David; Mackworth, Alan; Goebel, Randy (1998). Computational Intelligence: A Logical\nApproach (https://archive.org/details/computationalint00pool). New York: Oxford University Press.\nISBN 978-0-19-510270-3. Archived (https://web.archive.org/web/20200726131436/https://archive.\norg/details/computationalint00pool) from the original on 26 July 2020. Retrieved 22 August 2020.\nRussell, Stuart J.; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (http://aima.cs.b\nerkeley.edu/) (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2.\nNils J. Nilsson, Introduction to Machine Learning (https://ai.stanford.edu/people/nilsson/mlbook.ht\nml) Archived (https://web.archive.org/web/20190816182600/http://ai.stanford.edu/people/nilsson/\nmlbook.html) 2019-08-16 at the Wayback Machine.\nTrevor Hastie, Robert Tibshirani and Jerome H. Friedman (2001). The Elements of Statistical\nLearning (https://web.stanford.edu/~hastie/ElemStatLearn/) Archived (https://web.archive.org/web/\n20131027220938/http://www-stat.stanford.edu/%7Etibs/ElemStatLearn//) 2013-10-27 at the\nWayback Machine, Springer. ISBN 0-387-95284-5.\nPedro Domingos (September 2015), The Master Algorithm, Basic Books, ISBN 978-0-465-06570-\n7\nIan H. Witten and Eibe Frank (2011). Data Mining: Practical machine learning tools and\ntechniques Morgan Kaufmann, 664pp., ISBN 978-0-12-374856-0.\nEthem Alpaydin (2004). Introduction to Machine Learning, MIT Press, ISBN 978-0-262-01243-0.\nDavid J. C. MacKay. Information Theory, Inference, and Learning Algorithms (http://www.inferenc\ne.phy.cam.ac.uk/mackay/itila/book.html) Archived (https://web.archive.org/web/20160217105359/\nhttp://www.inference.phy.cam.ac.uk/mackay/itila/book.html) 2016-02-17 at the Wayback Machine\nCambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1\nRichard O. Duda, Peter E. Hart, David G. Stork (2001) Pattern classification (2nd edition), Wiley,\nNew York, ISBN 0-471-05669-3.\nChristopher Bishop (1995). Neural Networks for Pattern Recognition, Oxford University Press.\nISBN 0-19-853864-2.\nStuart Russell & Peter Norvig, (2009). Artificial Intelligence – A Modern Approach (http://aima.cs.b\nerkeley.edu/) Archived (https://web.archive.org/web/20110228023805/http://aima.cs.berkeley.edu/)\n2011-02-28 at the Wayback Machine. Pearson, ISBN 9789332543515.\nRay Solomonoff, An Inductive Inference Machine, IRE Convention Record, Section on Information\nTheory, Part 2, pp., 56–62, 1957.\nSources\nFurther readingRay Solomonoff, An Inductive Inference Machine (http://world.std.com/~rjs/indinf56.pdf) Archived\n(https://web.archive.org/web/20110426161749/http://world.std.com/~rjs/indinf56.pdf) 2011-04-26\nat the Wayback Machine A privately circulated report from the 1956 Dartmouth Summer Research\nConference on AI.\nKevin P. Murphy (2021). Probabilistic Machine Learning: An Introduction (https://probml.github.io/p\nml-book/book1.html) Archived (https://web.archive.org/web/20210411153246/https://probml.githu\nb.io/pml-book/book1.html) 2021-04-11 at the Wayback Machine, MIT Press.\nInternational Machine Learning Society (https://web.archive.org/web/20171230081341/http://mach\ninelearning.org/)\nmloss (https://mloss.org/) is an academic database of open-source machine learning software.\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Machine_learning&oldid=1259422339\"\nExternal links\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "reader = PdfReader(\"Machine learning - Wikipedia.pdf\")\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f646f0e-0463-4e79-8f5f-3d1387b0af32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 13:14:47.939747: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-12-02 13:14:48.253102: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5da76bf-6e11-4847-8a52-a80fb0e4f293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Generating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4958ef38-c0c4-4327-b4da-b4094ccf60ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sample: {'input_ids': tensor([[37573,  4673,   198, 37573,  4673,   357,  5805,     8,   318,   257,\n          2214,   286,  2050,   287, 11666,  4430,  5213,   351,   262,  2478,\n           198,   392,  2050,   286, 13905, 16113,   326,   460,  2193,   422,\n          1366,   290,  2276,  1096,   284, 29587,  1366,    11,   290,  4145,\n           198,   525,   687,  8861,  1231,  7952,  7729,  3693,    16,    60,\n          8007,  1817,   287,   262,  2214,   286,  2769,  4673,   423,  3142,\n           198,   710,  1523,  7686,   284, 17341,   867,  2180, 10581,   287,\n          2854,  3693,    17,    60,   198,  5805,  7228,  3586,   287,   867,\n          7032,    11,  1390,  3288,  3303,  7587,    11,  3644,  5761,    11,\n          4046,   198, 26243,   653,    11,  3053, 25431,    11, 14510,    11,\n           290,  9007,  3693,    18,  7131,    19,    60,   383,  3586,   286,\n         10373,   284,  1597,   198,  1676, 22143,   318,  1900,   355, 33344,\n         23696,    13,   198, 48346,   290, 18069, 23989,   357,  6759, 10024,\n           605,  8300,     8,  5050, 28889,   262,   198,  9275,   602,   286,\n          4572,  4673,    13,  6060,  9691,   318,   257,  3519,  2214,   286,\n          2050,    11, 10759,   319, 39180,  2870,  1366,   198, 20930,   357,\n          1961,    32,     8,  2884,   555, 16668, 16149,  4673,  3693,    21,\n          7131,    22,    60,   198,  4863,   257, 16200, 28953,    11,  2192,\n          6702,  3376,   357, 44938,     8,  4673,  3769,   257,  9355,   198,\n          1640, 12059,  4572,  4673,    13,   198,   464,  3381,  4572,  4673,\n           373, 33115,   287, 23859,   416, 13514, 17100,    11,   281, 19764,\n          6538,   290, 29570,   287,   198,  1169,  2214,   286,  3644,  7776,\n           290, 11666,  4430,  3693,    23,  7131,    24,    60,   383,  6171,\n          5177,  2116,    12,   660,  8103,  9061,   198,  9776,   635,   973,\n           287,   428,   640,  2278,  3693,   940,  7131,  1157,    60,   198,\n          7003,   262, 14555,  4572,  4673,  2746,   373,  5495,   287,   262,\n         11445,    82,   618, 13514, 17100,   198,   259,  1151,   276,   257,\n          1430,   326, 10488,   262,  5442,  2863,   287,  2198,   364,   329,\n          1123,  1735,    11,   262,  2106,   286,   198, 30243,  4673, 11135,\n           736,   284,  4647,   286,  1692,  6227,   290,  3626,   284,  2050,\n          1692, 10870,   198, 14681,   274,  3693,  1065,    60,   554, 24977,\n            11,  5398, 23540,  3759,   679, 11848,  3199,   262,  1492,   383,\n         12275,   286,   198, 25267, 15759,    11,   287,   543,   339,  5495,\n           257, 16200, 17019,  4645,  7042,   416,  1728, 12213,  1871,   198,\n          1008,   303,  4778,  3693,  1485,    60,   679, 11848,   338,  2746,\n           286, 16890, 24986,   351,   530,  1194,   900,   257, 40641,   329,\n           703,   317,  3792,   198,   392,  4572,  4673, 16113,   670,   739,\n         13760,    11,   393, 11666, 16890,   973,   416,  9061,   284,   198,\n         10709,  5344,  1366,  3693,  1065,    60,  3819,  4837,   508,   423,\n          9713,  1692, 10870,  3341,  8639,   284,   198,  1169,  3660,  4572,\n          4673,  8514,   355,   880,    11,  1390,  2604,  6749, 15819, 10276,\n            82,   290, 11328,   198, 30464,   724,  5374,    11,   508,  5150,\n           262,  1903, 18069,  4981,   286, 17019,  7686,   284,  1282,   510,\n           351,   198,   282,  7727,   907,   326, 10162,  1692,  1807,  7767,\n          3693,  1065,    60,   198,  3886,   262,  1903,  9507,    82,    11,\n           281, 11992,   366, 40684,  4572,     1,   351, 25436,  9154,  4088,\n            11,  1444,  5934,  4835,  1313,    11,   198, 18108,   587,  4166,\n           416,  7760,  1169,   261,  5834,   284, 16602,  3367,   283, 10425,\n            11, 15206,  9517,    72, 26836,    11,   290,  4046,   198, 33279,\n            82,  1262, 47381, 37414,  4673,    13,   632,   373, 28585,   306,\n           366, 35311,     1,   416,   257,  1692,   198, 46616,    14,   660,\n          3493,   284]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "print(\"Tokenized Sample:\", inputs)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Extract Pdf & Generate Embeddings",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
